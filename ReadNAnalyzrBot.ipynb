{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da68a66c",
   "metadata": {},
   "source": [
    "# ReadNAnalyzrBot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8112e09",
   "metadata": {},
   "source": [
    "## Installing and Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a98487fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instal the required modules:\n",
    "#pip install textblob\n",
    "#pip install beautifulsoup4\n",
    "#pip install nltk\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import chardet\n",
    "import string\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d43a107",
   "metadata": {},
   "source": [
    "## Load input Excel file that contains URL of the sights you need to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6102fbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>blackassign0096</td>\n",
       "      <td>https://insights.blackcoffer.com/what-is-the-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>blackassign0097</td>\n",
       "      <td>https://insights.blackcoffer.com/impact-of-cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>blackassign0098</td>\n",
       "      <td>https://insights.blackcoffer.com/contribution-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>blackassign0099</td>\n",
       "      <td>https://insights.blackcoffer.com/how-covid-19-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>blackassign0100</td>\n",
       "      <td>https://insights.blackcoffer.com/how-will-covi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             URL_ID                                                URL\n",
       "0   blackassign0001  https://insights.blackcoffer.com/rising-it-cit...\n",
       "1   blackassign0002  https://insights.blackcoffer.com/rising-it-cit...\n",
       "2   blackassign0003  https://insights.blackcoffer.com/internet-dema...\n",
       "3   blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...\n",
       "4   blackassign0005  https://insights.blackcoffer.com/ott-platform-...\n",
       "..              ...                                                ...\n",
       "95  blackassign0096  https://insights.blackcoffer.com/what-is-the-r...\n",
       "96  blackassign0097  https://insights.blackcoffer.com/impact-of-cov...\n",
       "97  blackassign0098  https://insights.blackcoffer.com/contribution-...\n",
       "98  blackassign0099  https://insights.blackcoffer.com/how-covid-19-...\n",
       "99  blackassign0100  https://insights.blackcoffer.com/how-will-covi...\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file_location = r'D:\\DATA ANALYST PLAYGROUND\\Blackcoffer\\Input.xlsx'\n",
    "input_file = pd.read_excel(input_file_location)\n",
    "input_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c50cfa5",
   "metadata": {},
   "source": [
    "## Extracting texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "dab6ce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_text(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extracting article title\n",
    "    title_element_1 = soup.find('h1', class_='entry-title')\n",
    "    title_element_2 = soup.find('h1', class_='tdb-title-text')\n",
    "\n",
    "    # Use the title from the first found element (non-empty)\n",
    "    title = title_element_1.text.strip() if title_element_1 else title_element_2.text.strip() if title_element_2 else \"\"\n",
    "    \n",
    "    # Extracting article text \n",
    "    article_text_element_1 = soup.find(class_=\"td-post-content tagdiv-type\")\n",
    "\n",
    "    if article_text_element_1:\n",
    "        # Ignore text inside \"wp-block-preformatted\" class\n",
    "        preformatted_elements = article_text_element_1.find_all(class_=\"wp-block-preformatted\")\n",
    "        for preformatted_element in preformatted_elements:\n",
    "            preformatted_element.decompose()  # Remove the element and its content\n",
    "        \n",
    "        # Extract text after removing \"wp-block-preformatted\" elements\n",
    "        article_text = article_text_element_1.text.strip()\n",
    "    else:\n",
    "        # If not found, try to find text inside <p>, <li>, and <u> tags in \"tdb-block-inner td-fix-index\"\n",
    "        tdb_block_inner_elements = soup.find_all(class_=\"tdb-block-inner td-fix-index\")\n",
    "        combined_texts = []\n",
    "\n",
    "        for tdb_block_inner in tdb_block_inner_elements:\n",
    "            # Extract text from <p>, <li>, and <u> tags\n",
    "            p_elements = tdb_block_inner.find_all(['p', 'li', 'u'])\n",
    "            combined_texts.extend([element.text.strip() for element in p_elements if element.text.strip()])\n",
    "\n",
    "        article_text = ' '.join(combined_texts)\n",
    "\n",
    "    article_text = re.sub(r'\\n', '', article_text)\n",
    "\n",
    "    \n",
    "    return title, article_text\n",
    "\n",
    "\n",
    "\n",
    "#\"td-post-content tagdiv-type\"\n",
    "#\"tdb-block-inner td-fix-index\"\n",
    "#\"tdb-title-text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "0a2fa70e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('How will COVID-19 affect the world of work?',\n",
       " 'As business close to help prevent transmission of COVID-19, financial concerns and job losses are one of the first human impacts of the virus; Not knowing how this pandemic will play out also affects our economic, physical and mental well-being; Despite this fear, businesses and communities in many regions have shown a more altruistic response in the face of crisis – actions which could help countries preparing for COVID-19. COVID-19 is in decline in China. There are now more new cases every day in Europe than there were in China at the epidemic’s peak and Italy has surpassed it as the country with the most deaths from the virus It took 67 days to reach the first 100,000 confirmed cases worldwide, 11 days for this to increase to 200,000and just four to reach 300,000 confirmed cases – a figure now exceeded. In recent weeks, we have seen the significant economic impact of the coronavirus on financial markets and vulnerable industries such as manufacturing, tourism, hospitality and travel.\\xa0Travel and tourism account for 10% of the global GDP and 50 million jobs are at risk worldwide. Global tourism, travel and hospitality companies closing down affects SMEs globally. This, in turn, affects many people, typically the least well-paid and those self-employed or working in informal environments in the gig economy or in part-time work with zero-hours contracts. Some governments have announced economic measures to safeguard jobs, guarantee wages and support the self-employed, but there is a lack of clarity in many countries about how these measures will be implemented and how people will manage a loss of income in the short-term. Behind these statistics lie the human costs of the pandemic, from the deaths of friends and family to the physical effects of infection and the mental trauma and fear faced by almost everyone. Not knowing how this pandemic will play out affects our economic, physical and mental well-being against a backdrop of a world that, for many, is increasingly anxious, unhappy and lonely. Fear of the unknown can often lead to feelings of panic, for example when people feel they are being denied life-saving protection or treatment or that they may run out of necessities, which can lead to panic buying.Psychological stress is often related to a sense of a lack of control in the face of uncertainty. In all cases, lack of information or the wrong information, either provided inadvertently or maliciously, can amplify the effects. There is a huge amount of misleading information circulating online about COVID-19, from fake medical information to speculation about government responses. People are susceptible to social media posts from an apparently trustworthy source, often referred to as an “Uncle with a Masters”-post, possibly amplified and spread by “copypasta” posts, which share information by copying and pasting and make each new post look like an original source, as opposed to posts that are “liked” or “shared” or “retweeted”. Sadly, criminals and hackers are also exploiting this situation and there has been a significant rise in Coronavirus-themed malicious websites, with more than 16,000 new coronavirus-related domains registered since January 2020. Hackers are selling malware and hacking tools through COVID-19 discount codes on the darknet,many of which are aimed at accessing corporate data from home-workers’ laptops, which may not be as secure as outside an office environment. Social distancing and lockdowns have also prompted altruistic behaviors, in part because of the sense that “we’re all in this together”. Many people report being bored or concerned about putting on weight;\\xa0others have discovered a slower pace of life and by not going out and socializing have found more time for family, others, and even their pets. The downside of self-isolation or social lockdown are symptoms of traumatic stress, confusion and anger, all of which are exacerbated by fear of infection, having limited access to supplies of necessities, inadequate information or the experience of economic loss or stigma. This stress and anxiety can lead to increased alcohol consumption, as well as an increase in domestic and family violence.In Jingzhou, a town near Wuhan in Hubei province, reports of domestic violence during the lockdown in February 2020 were more than triple the number reported in February 2019. Health measures must be the first priority for governments, business and society. It is important for businesses to show solidarity and work together to protect staff, local communities and customers, as well as keeping supply chains, manufacturing and logistics working.According to research, “my employer” is more trusted than the government or media. Daily updates on a company website with input from scientists and experts are recommended to counter politicized messages in the media and from governments. This is particularly true for large companies that have the capacity to do this. Messages about what businesses are doing for their employees and in their communities is also important. Some companies are helping schoolchildren from vulnerable families who can no longer get a school meal; others are providing public health messages about effective handwashing. Even CEOs can show they are working from home and self-isolating, while still being effective in their leadership. Following WHO advice, there is a need for the business community to move from general support to specific actions and focus on countries’ access to critical supplies, including a “Community Package of Critical Items” (a list of 46 items that all countries need). Of these items, 20 are either not available locally or available stocks are too limited. These missing items fall into four categories: Hygiene: Chlorine, HTH 70%, alcohol based hand rub, liquid soap; Diagnostics: lab screening tests, lab confirmation tests, enzymes, RNA extraction kits; PPE: gowns, scrubs, aprons, sterile gloves, protective goggles, face shields, masks (N95 or FFP2); Case management equipment: oxygen concentrators, oxygen delivery systems, mechanical ventilators. The call for action is for more money, to work with manufacturers to create capacity and to organize purchasing so there is guaranteed access, especially for poorer countries with less resilient public health systems. The concept is to create a global security stockpile of supplies and equipment, an effort that needs: Emergency financing Access to and increases in manufacturing capacity Access to national and supplier stockpiles Warehouses and distribution capacity')"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_article_text(r'https://insights.blackcoffer.com/how-will-covid-19-affect-the-world-of-work-2/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeadb229",
   "metadata": {},
   "source": [
    "## Creating txt files out of extracted text content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cdcb709c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[140], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m url \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURL\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Extract article text and title from the URL\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m title, article_text \u001b[38;5;241m=\u001b[39m extract_article_text(url)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Save the extracted title and text in a text file\u001b[39;00m\n\u001b[0;32m     10\u001b[0m output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDATA ANALYST PLAYGROUND\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mBlackcoffer\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mExtracted Text files\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[135], line 2\u001b[0m, in \u001b[0;36mextract_article_text\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_article_text\u001b[39m(url):\n\u001b[1;32m----> 2\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m      3\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Extracting article title\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    715\u001b[0m     conn,\n\u001b[0;32m    716\u001b[0m     method,\n\u001b[0;32m    717\u001b[0m     url,\n\u001b[0;32m    718\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    719\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    720\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    721\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    722\u001b[0m )\n\u001b[0;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[0;32m    728\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    461\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    462\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m             six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 461\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1378\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1308\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1309\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1310\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for index, row in input_file.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    \n",
    "    # Extract article text and title from the URL\n",
    "    title, article_text = extract_article_text(url)\n",
    "    \n",
    "    # Save the extracted title and text in a text file\n",
    "    output_folder = r'D:\\DATA ANALYST PLAYGROUND\\Blackcoffer\\Extracted Text files'\n",
    "    with open(os.path.join(output_folder, f'{url_id}.txt'), 'w', encoding='utf-8') as file:\n",
    "        file.write(f'{title}\\n\\n')\n",
    "        file.write(f' {article_text}\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe79c04",
   "metadata": {},
   "source": [
    "## Functions to perform different text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b08faa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_sentiment_analysis(text):\n",
    "   \n",
    "    full_text = text\n",
    "    # Cleaning using Stop Words Lists\n",
    "    stop_words_folder = r'D:\\DATA ANALYST PLAYGROUND\\Blackcoffer\\StopWords'\n",
    "    stop_words = set()\n",
    "\n",
    "    for filename in os.listdir(stop_words_folder):\n",
    "        file_path = os.path.join(stop_words_folder, filename)\n",
    "        with open(file_path, 'rb') as file:\n",
    "            raw_data = file.read()\n",
    "            encoding_result = chardet.detect(raw_data)\n",
    "            encoding = encoding_result['encoding']\n",
    "\n",
    "        with open(file_path, 'r', encoding=encoding, errors='replace') as file:\n",
    "            stop_words.update(file.read().split())\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(full_text.lower())\n",
    "    cleaned_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "    print(\"Word count of the original article: \", len(tokens))\n",
    "    print(\"Word count of the cleaned article: \", len(cleaned_tokens))\n",
    "\n",
    "    # Creating a dictionary of Positive and Negative words\n",
    "    positive_words = set(word for word in open(r'D:\\DATA ANALYST PLAYGROUND\\Blackcoffer\\MasterDictionary\\positive-words.txt').read().split() if word not in stop_words)\n",
    "    negative_words = set(word for word in open(r'D:\\DATA ANALYST PLAYGROUND\\Blackcoffer\\MasterDictionary/negative-words.txt').read().split() if word not in stop_words)\n",
    "\n",
    "    # Extracting Derived variables\n",
    "    positive_score = sum(1 for word in cleaned_tokens if word in positive_words)\n",
    "    negative_score = sum(1 for word in cleaned_tokens if word in negative_words)\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score) / (len(cleaned_tokens) + 0.000001)\n",
    "\n",
    "    \n",
    "    # Return the results\n",
    "    return {\n",
    "        'POSITIVE SCORE': positive_score,\n",
    "        'NEGATIVE SCORE': negative_score,\n",
    "        'POLARITY SCORE': polarity_score,\n",
    "        'SUBJECTIVITY SCORE': subjectivity_score,\n",
    "    \n",
    "        # Add other variables as needed\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2001c4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count of the original article:  1343\n",
      "Word count of the cleaned article:  681\n",
      "{'POSITIVE SCORE': 33, 'NEGATIVE SCORE': 5, 'POLARITY SCORE': 0.7368420858725767, 'SUBJECTIVITY SCORE': 0.05580029360381748, 'AVG SENTENCE LENGTH': 22.45, 'PERCENTAGE OF COMPLEX WORDS': 17.72151898734177}\n"
     ]
    }
   ],
   "source": [
    "url = r'https://insights.blackcoffer.com/rising-it-cities-and-its-impact-on-the-economy-environment-infrastructure-and-city-life-by-the-year-2040-2/'\n",
    "title, article_text = extract_article_text(url)\n",
    "sentiment_analysis_results = perform_sentiment_analysis(title, article_text)\n",
    "print(analysis_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f9571146",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Function to count syllables in a word\n",
    "    def count_syllables(word):\n",
    "        vowels = \"aeiou\"\n",
    "        count = 0\n",
    "\n",
    "        # Iterate through each letter in the word\n",
    "        for char in word:\n",
    "            # Count consecutive vowels\n",
    "            if char.lower() in vowels:\n",
    "                count += 1\n",
    "\n",
    "        # Exclude words ending with \"es\" and \"ed\" from syllable count\n",
    "        if word.lower().endswith(('es', 'ed')):\n",
    "            count -= 1\n",
    "\n",
    "        # Ensure a minimum count of 1\n",
    "        return max(count, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a6af2fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_readability_analysis(text):\n",
    "    \n",
    "    full_text = text\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(full_text.lower())\n",
    "\n",
    "    # let's calculate average sentence length\n",
    "    sentences = sent_tokenize(full_text)\n",
    "    avg_sentence_length = len(tokens) / len(sentences) if len(sentences) > 0 else 0\n",
    "\n",
    "    # Count syllables for each word\n",
    "    syllables_per_word = [count_syllables(word) for word in tokens]\n",
    "    #print(syllables_per_word)\n",
    "\n",
    "    # Calculate Percentage of Complex Words based on syllables\n",
    "    complex_words = [word for word, syllables in zip(tokens, syllables_per_word) if syllables > 2]\n",
    "    percentage_complex_words = (len(complex_words) / len(tokens)) * 100 if len(tokens) > 0 else 0\n",
    "    \n",
    "    # Calculate fog index\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "\n",
    "\n",
    "   \n",
    "    return {\n",
    "        'AVG SENTENCE LENGTH': avg_sentence_length,\n",
    "        'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words,\n",
    "        'FOG INDEX': fog_index\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "02730e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AVG SENTENCE LENGTH': 22.166666666666668, 'PERCENTAGE OF COMPLEX WORDS': 17.593984962406015, 'FOG INDEX': 15.904260651629073}\n"
     ]
    }
   ],
   "source": [
    "url = r'https://insights.blackcoffer.com/rising-it-cities-and-its-impact-on-the-economy-environment-infrastructure-and-city-life-by-the-year-2040-2/'\n",
    "title, article_text = extract_article_text(url)\n",
    "full_text = f\"{title} {article_text}\"\n",
    "\n",
    "readability_analysis_results = perform_readability_analysis(full_text)\n",
    "print(readability_analysis_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "56ef349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_basic_analysis(text):\n",
    "    \n",
    "    full_text = text\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(full_text.lower())\n",
    "    \n",
    "    #AVERAGE WORD PER SENTENCE\n",
    "    sentences = sent_tokenize(full_text)\n",
    "    avg_words_per_sentence = len(tokens)/len(sentences) if len(sentences) > 0 else 0\n",
    "    syllables_per_word = [count_syllables(word) for word in tokens]\n",
    "    complex_words = [word for word, syllables in zip(tokens, syllables_per_word) if syllables > 2]\n",
    "    complex_wordcount = len(complex_words)\n",
    "    \n",
    "    stop_words2 = set(stopwords.words('english'))\n",
    "    cleaned_tokens2 = [word for word in tokens if word.lower() not in stop_words2 and word not in string.punctuation]\n",
    "    word_count = len(cleaned_tokens2)\n",
    "    syllables_per_word = [count_syllables(word) for word in tokens]\n",
    "    average_syllable_count = sum(syllables_per_word) / len(syllables_per_word) if len(syllables_per_word) > 0 else 0\n",
    "    \n",
    "    # Define the list of personal pronouns\n",
    "    personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n",
    "\n",
    "    # Use regex to find counts of personal pronouns\n",
    "    total_pronoun_count = sum(len(re.findall(r'\\b' + pronoun + r'\\b', full_text, flags=re.IGNORECASE)) for pronoun in personal_pronouns)\n",
    " \n",
    "    # Special care to exclude the country name \"US\"\n",
    "    total_pronoun_count -= len(re.findall(r'\\bUS\\b', full_text, flags=re.IGNORECASE))\n",
    "\n",
    "    \n",
    "    total_characters = sum(len(word) for word in tokens)\n",
    "\n",
    "    # Calculate the total number of words\n",
    "    total_words = len(tokens)\n",
    "\n",
    "    # Calculate the average word length\n",
    "    average_word_length = total_characters / total_words if total_words > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'AVERAGE WORD PER SENTENCE': avg_words_per_sentence,\n",
    "        'COMPLEX WORD COUNT': complex_wordcount,\n",
    "        'WORD COUNT': word_count,\n",
    "        'AVERAGE SYLLABLE COUNT':average_syllable_count,\n",
    "        'PRONOUN COUNT':total_pronoun_count,\n",
    "        'AVERAGE WORD LENGTH':average_word_length\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "7d664588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AVERAGE WORD PER SENTENCE': 22.166666666666668, 'COMPLEX WORD COUNT': 234, 'WORD COUNT': 626, 'AVERAGE SYLLABLE COUNT': 1.700751879699248, 'PRONOUN COUNT': 11, 'AVERAGE WORD LENGTH': 4.327067669172933}\n"
     ]
    }
   ],
   "source": [
    "url = r'https://insights.blackcoffer.com/rising-it-cities-and-its-impact-on-the-economy-environment-infrastructure-and-city-life-by-the-year-2040-2/'\n",
    "title, article_text = extract_article_text(url)\n",
    "full_text = f\"{title} {article_text}\"\n",
    "\n",
    "basic_analysis_results = perform_basic_analysis(full_text)\n",
    "print(basic_analysis_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea033106",
   "metadata": {},
   "source": [
    "## Writing the text analysis score on the output sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "1a5c09c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count of the original article:  1330\n",
      "Word count of the cleaned article:  670\n",
      "Total number of cleaned words 670\n",
      "Total number of sentences 60\n",
      "Word count of the original article:  1668\n",
      "Word count of the cleaned article:  988\n",
      "Total number of cleaned words 988\n",
      "Total number of sentences 64\n",
      "Word count of the original article:  1194\n",
      "Word count of the cleaned article:  755\n",
      "Total number of cleaned words 755\n",
      "Total number of sentences 46\n",
      "Word count of the original article:  1181\n",
      "Word count of the cleaned article:  733\n",
      "Total number of cleaned words 733\n",
      "Total number of sentences 38\n",
      "Word count of the original article:  742\n",
      "Word count of the cleaned article:  428\n",
      "Total number of cleaned words 428\n",
      "Total number of sentences 28\n",
      "Word count of the original article:  2023\n",
      "Word count of the cleaned article:  1274\n",
      "Total number of cleaned words 1274\n",
      "Total number of sentences 62\n",
      "Word count of the original article:  1419\n",
      "Word count of the cleaned article:  820\n",
      "Total number of cleaned words 820\n",
      "Total number of sentences 51\n",
      "Word count of the original article:  873\n",
      "Word count of the cleaned article:  585\n",
      "Total number of cleaned words 585\n",
      "Total number of sentences 44\n",
      "Word count of the original article:  1058\n",
      "Word count of the cleaned article:  713\n",
      "Total number of cleaned words 713\n",
      "Total number of sentences 39\n",
      "Word count of the original article:  3964\n",
      "Word count of the cleaned article:  2098\n",
      "Total number of cleaned words 2098\n",
      "Total number of sentences 155\n",
      "Word count of the original article:  1550\n",
      "Word count of the cleaned article:  937\n",
      "Total number of cleaned words 937\n",
      "Total number of sentences 50\n",
      "Word count of the original article:  1796\n",
      "Word count of the cleaned article:  995\n",
      "Total number of cleaned words 995\n",
      "Total number of sentences 56\n",
      "Word count of the original article:  650\n",
      "Word count of the cleaned article:  327\n",
      "Total number of cleaned words 327\n",
      "Total number of sentences 19\n",
      "Word count of the original article:  1226\n",
      "Word count of the cleaned article:  686\n",
      "Total number of cleaned words 686\n",
      "Total number of sentences 69\n",
      "Word count of the original article:  1336\n",
      "Word count of the cleaned article:  761\n",
      "Total number of cleaned words 761\n",
      "Total number of sentences 46\n",
      "Word count of the original article:  1336\n",
      "Word count of the cleaned article:  761\n",
      "Total number of cleaned words 761\n",
      "Total number of sentences 46\n",
      "Word count of the original article:  1262\n",
      "Word count of the cleaned article:  725\n",
      "Total number of cleaned words 725\n",
      "Total number of sentences 45\n",
      "Word count of the original article:  1180\n",
      "Word count of the cleaned article:  693\n",
      "Total number of cleaned words 693\n",
      "Total number of sentences 33\n",
      "Word count of the original article:  1888\n",
      "Word count of the cleaned article:  1020\n",
      "Total number of cleaned words 1020\n",
      "Total number of sentences 74\n",
      "Word count of the original article:  611\n",
      "Word count of the cleaned article:  279\n",
      "Total number of cleaned words 279\n",
      "Total number of sentences 27\n",
      "Word count of the original article:  1144\n",
      "Word count of the cleaned article:  653\n",
      "Total number of cleaned words 653\n",
      "Total number of sentences 41\n",
      "Word count of the original article:  453\n",
      "Word count of the cleaned article:  218\n",
      "Total number of cleaned words 218\n",
      "Total number of sentences 24\n",
      "Word count of the original article:  1515\n",
      "Word count of the cleaned article:  807\n",
      "Total number of cleaned words 807\n",
      "Total number of sentences 55\n",
      "Word count of the original article:  624\n",
      "Word count of the cleaned article:  358\n",
      "Total number of cleaned words 358\n",
      "Total number of sentences 16\n",
      "Word count of the original article:  1148\n",
      "Word count of the cleaned article:  611\n",
      "Total number of cleaned words 611\n",
      "Total number of sentences 21\n",
      "Word count of the original article:  1002\n",
      "Word count of the cleaned article:  600\n",
      "Total number of cleaned words 600\n",
      "Total number of sentences 36\n",
      "Word count of the original article:  1464\n",
      "Word count of the cleaned article:  708\n",
      "Total number of cleaned words 708\n",
      "Total number of sentences 49\n",
      "Word count of the original article:  1243\n",
      "Word count of the cleaned article:  695\n",
      "Total number of cleaned words 695\n",
      "Total number of sentences 33\n",
      "Word count of the original article:  2007\n",
      "Word count of the cleaned article:  1237\n",
      "Total number of cleaned words 1237\n",
      "Total number of sentences 76\n",
      "Word count of the original article:  1618\n",
      "Word count of the cleaned article:  814\n",
      "Total number of cleaned words 814\n",
      "Total number of sentences 68\n",
      "Word count of the original article:  1873\n",
      "Word count of the cleaned article:  1046\n",
      "Total number of cleaned words 1046\n",
      "Total number of sentences 68\n",
      "Word count of the original article:  1787\n",
      "Word count of the cleaned article:  869\n",
      "Total number of cleaned words 869\n",
      "Total number of sentences 79\n",
      "Word count of the original article:  1947\n",
      "Word count of the cleaned article:  1030\n",
      "Total number of cleaned words 1030\n",
      "Total number of sentences 68\n",
      "Word count of the original article:  1452\n",
      "Word count of the cleaned article:  745\n",
      "Total number of cleaned words 745\n",
      "Total number of sentences 47\n",
      "Word count of the original article:  832\n",
      "Word count of the cleaned article:  448\n",
      "Total number of cleaned words 448\n",
      "Total number of sentences 41\n",
      "Word count of the original article:  0\n",
      "Word count of the cleaned article:  0\n",
      "Total number of cleaned words 0\n",
      "Total number of sentences 0\n",
      "Word count of the original article:  803\n",
      "Word count of the cleaned article:  417\n",
      "Total number of cleaned words 417\n",
      "Total number of sentences 30\n",
      "Word count of the original article:  2381\n",
      "Word count of the cleaned article:  1214\n",
      "Total number of cleaned words 1214\n",
      "Total number of sentences 69\n",
      "Word count of the original article:  2340\n",
      "Word count of the cleaned article:  1343\n",
      "Total number of cleaned words 1343\n",
      "Total number of sentences 88\n",
      "Word count of the original article:  1276\n",
      "Word count of the cleaned article:  705\n",
      "Total number of cleaned words 705\n",
      "Total number of sentences 41\n",
      "Word count of the original article:  1391\n",
      "Word count of the cleaned article:  786\n",
      "Total number of cleaned words 786\n",
      "Total number of sentences 65\n",
      "Word count of the original article:  1777\n",
      "Word count of the cleaned article:  797\n",
      "Total number of cleaned words 797\n",
      "Total number of sentences 33\n",
      "Word count of the original article:  1800\n",
      "Word count of the cleaned article:  984\n",
      "Total number of cleaned words 984\n",
      "Total number of sentences 84\n",
      "Word count of the original article:  559\n",
      "Word count of the cleaned article:  323\n",
      "Total number of cleaned words 323\n",
      "Total number of sentences 18\n",
      "Word count of the original article:  1888\n",
      "Word count of the cleaned article:  803\n",
      "Total number of cleaned words 803\n",
      "Total number of sentences 8\n",
      "Word count of the original article:  1027\n",
      "Word count of the cleaned article:  552\n",
      "Total number of cleaned words 552\n",
      "Total number of sentences 33\n",
      "Word count of the original article:  781\n",
      "Word count of the cleaned article:  446\n",
      "Total number of cleaned words 446\n",
      "Total number of sentences 21\n",
      "Word count of the original article:  331\n",
      "Word count of the cleaned article:  169\n",
      "Total number of cleaned words 169\n",
      "Total number of sentences 2\n",
      "Word count of the original article:  0\n",
      "Word count of the cleaned article:  0\n",
      "Total number of cleaned words 0\n",
      "Total number of sentences 0\n",
      "Word count of the original article:  1502\n",
      "Word count of the cleaned article:  815\n",
      "Total number of cleaned words 815\n",
      "Total number of sentences 42\n",
      "Word count of the original article:  796\n",
      "Word count of the cleaned article:  427\n",
      "Total number of cleaned words 427\n",
      "Total number of sentences 24\n",
      "Word count of the original article:  2151\n",
      "Word count of the cleaned article:  1039\n",
      "Total number of cleaned words 1039\n",
      "Total number of sentences 89\n",
      "Word count of the original article:  726\n",
      "Word count of the cleaned article:  389\n",
      "Total number of cleaned words 389\n",
      "Total number of sentences 29\n",
      "Word count of the original article:  415\n",
      "Word count of the cleaned article:  227\n",
      "Total number of cleaned words 227\n",
      "Total number of sentences 14\n",
      "Word count of the original article:  1227\n",
      "Word count of the cleaned article:  677\n",
      "Total number of cleaned words 677\n",
      "Total number of sentences 27\n",
      "Word count of the original article:  659\n",
      "Word count of the cleaned article:  393\n",
      "Total number of cleaned words 393\n",
      "Total number of sentences 13\n",
      "Word count of the original article:  426\n",
      "Word count of the cleaned article:  196\n",
      "Total number of cleaned words 196\n",
      "Total number of sentences 24\n",
      "Word count of the original article:  173\n",
      "Word count of the cleaned article:  100\n",
      "Total number of cleaned words 100\n",
      "Total number of sentences 10\n",
      "Word count of the original article:  945\n",
      "Word count of the cleaned article:  460\n",
      "Total number of cleaned words 460\n",
      "Total number of sentences 56\n",
      "Word count of the original article:  199\n",
      "Word count of the cleaned article:  101\n",
      "Total number of cleaned words 101\n",
      "Total number of sentences 6\n",
      "Word count of the original article:  1263\n",
      "Word count of the cleaned article:  744\n",
      "Total number of cleaned words 744\n",
      "Total number of sentences 36\n",
      "Word count of the original article:  593\n",
      "Word count of the cleaned article:  308\n",
      "Total number of cleaned words 308\n",
      "Total number of sentences 20\n",
      "Word count of the original article:  1141\n",
      "Word count of the cleaned article:  546\n",
      "Total number of cleaned words 546\n",
      "Total number of sentences 47\n",
      "Word count of the original article:  1919\n",
      "Word count of the cleaned article:  1053\n",
      "Total number of cleaned words 1053\n",
      "Total number of sentences 51\n",
      "Word count of the original article:  1853\n",
      "Word count of the cleaned article:  866\n",
      "Total number of cleaned words 866\n",
      "Total number of sentences 46\n",
      "Word count of the original article:  1654\n",
      "Word count of the cleaned article:  894\n",
      "Total number of cleaned words 894\n",
      "Total number of sentences 71\n",
      "Word count of the original article:  972\n",
      "Word count of the cleaned article:  428\n",
      "Total number of cleaned words 428\n",
      "Total number of sentences 54\n",
      "Word count of the original article:  1610\n",
      "Word count of the cleaned article:  694\n",
      "Total number of cleaned words 694\n",
      "Total number of sentences 63\n",
      "Word count of the original article:  523\n",
      "Word count of the cleaned article:  226\n",
      "Total number of cleaned words 226\n",
      "Total number of sentences 14\n",
      "Word count of the original article:  1749\n",
      "Word count of the cleaned article:  972\n",
      "Total number of cleaned words 972\n",
      "Total number of sentences 42\n",
      "Word count of the original article:  1134\n",
      "Word count of the cleaned article:  527\n",
      "Total number of cleaned words 527\n",
      "Total number of sentences 65\n",
      "Word count of the original article:  1309\n",
      "Word count of the cleaned article:  623\n",
      "Total number of cleaned words 623\n",
      "Total number of sentences 60\n",
      "Word count of the original article:  1277\n",
      "Word count of the cleaned article:  724\n",
      "Total number of cleaned words 724\n",
      "Total number of sentences 31\n",
      "Word count of the original article:  1585\n",
      "Word count of the cleaned article:  891\n",
      "Total number of cleaned words 891\n",
      "Total number of sentences 45\n",
      "Word count of the original article:  1195\n",
      "Word count of the cleaned article:  564\n",
      "Total number of cleaned words 564\n",
      "Total number of sentences 68\n",
      "Word count of the original article:  1710\n",
      "Word count of the cleaned article:  855\n",
      "Total number of cleaned words 855\n",
      "Total number of sentences 44\n",
      "Word count of the original article:  839\n",
      "Word count of the cleaned article:  462\n",
      "Total number of cleaned words 462\n",
      "Total number of sentences 21\n",
      "Word count of the original article:  1309\n",
      "Word count of the cleaned article:  755\n",
      "Total number of cleaned words 755\n",
      "Total number of sentences 29\n",
      "Word count of the original article:  1875\n",
      "Word count of the cleaned article:  1006\n",
      "Total number of cleaned words 1006\n",
      "Total number of sentences 47\n",
      "Word count of the original article:  3886\n",
      "Word count of the cleaned article:  2571\n",
      "Total number of cleaned words 2571\n",
      "Total number of sentences 163\n",
      "Word count of the original article:  1891\n",
      "Word count of the cleaned article:  957\n",
      "Total number of cleaned words 957\n",
      "Total number of sentences 62\n",
      "Word count of the original article:  1668\n",
      "Word count of the cleaned article:  999\n",
      "Total number of cleaned words 999\n",
      "Total number of sentences 49\n",
      "Word count of the original article:  162\n",
      "Word count of the cleaned article:  76\n",
      "Total number of cleaned words 76\n",
      "Total number of sentences 7\n",
      "Word count of the original article:  1123\n",
      "Word count of the cleaned article:  609\n",
      "Total number of cleaned words 609\n",
      "Total number of sentences 43\n",
      "Word count of the original article:  2005\n",
      "Word count of the cleaned article:  896\n",
      "Total number of cleaned words 896\n",
      "Total number of sentences 105\n",
      "Word count of the original article:  1968\n",
      "Word count of the cleaned article:  1049\n",
      "Total number of cleaned words 1049\n",
      "Total number of sentences 72\n",
      "Word count of the original article:  1163\n",
      "Word count of the cleaned article:  623\n",
      "Total number of cleaned words 623\n",
      "Total number of sentences 36\n",
      "Word count of the original article:  1943\n",
      "Word count of the cleaned article:  1160\n",
      "Total number of cleaned words 1160\n",
      "Total number of sentences 44\n",
      "Word count of the original article:  1059\n",
      "Word count of the cleaned article:  575\n",
      "Total number of cleaned words 575\n",
      "Total number of sentences 35\n",
      "Word count of the original article:  1337\n",
      "Word count of the cleaned article:  641\n",
      "Total number of cleaned words 641\n",
      "Total number of sentences 68\n",
      "Word count of the original article:  1161\n",
      "Word count of the cleaned article:  553\n",
      "Total number of cleaned words 553\n",
      "Total number of sentences 30\n",
      "Word count of the original article:  1468\n",
      "Word count of the cleaned article:  909\n",
      "Total number of cleaned words 909\n",
      "Total number of sentences 58\n",
      "Word count of the original article:  206\n",
      "Word count of the cleaned article:  99\n",
      "Total number of cleaned words 99\n",
      "Total number of sentences 6\n",
      "Word count of the original article:  1265\n",
      "Word count of the cleaned article:  671\n",
      "Total number of cleaned words 671\n",
      "Total number of sentences 54\n",
      "Word count of the original article:  701\n",
      "Word count of the cleaned article:  420\n",
      "Total number of cleaned words 420\n",
      "Total number of sentences 18\n",
      "Word count of the original article:  1203\n",
      "Word count of the cleaned article:  660\n",
      "Total number of cleaned words 660\n",
      "Total number of sentences 45\n",
      "Word count of the original article:  1183\n",
      "Word count of the cleaned article:  527\n",
      "Total number of cleaned words 527\n",
      "Total number of sentences 27\n",
      "Word count of the original article:  425\n",
      "Word count of the cleaned article:  274\n",
      "Total number of cleaned words 274\n",
      "Total number of sentences 9\n",
      "Word count of the original article:  684\n",
      "Word count of the cleaned article:  372\n",
      "Total number of cleaned words 372\n",
      "Total number of sentences 33\n",
      "Word count of the original article:  1175\n",
      "Word count of the cleaned article:  667\n",
      "Total number of cleaned words 667\n",
      "Total number of sentences 33\n"
     ]
    }
   ],
   "source": [
    "# Load the \"Output Data Structure\" Excel sheet into a DataFrame\n",
    "output_excel_path = r'D:\\DATA ANALYST PLAYGROUND\\Blackcoffer\\Output Data Structure.xlsx'\n",
    "df = pd.read_excel(output_excel_path)\n",
    "\n",
    "# Folder path containing your text files\n",
    "extracted_txt_folder_path = r'D:\\DATA ANALYST PLAYGROUND\\Blackcoffer\\Extracted Text files'\n",
    "\n",
    "# Iterate through rows of the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Extract URL_ID and construct file path\n",
    "    url_id = row['URL_ID']\n",
    "    txt_file_path = os.path.join(extracted_txt_folder_path, f'{url_id}.txt')\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if os.path.isfile(txt_file_path):\n",
    "        # Read the text from the file\n",
    "        with open(txt_file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            \n",
    "        # Perform sentiment analysis\n",
    "        sentiment_analysis_results = perform_sentiment_analysis(text)\n",
    "        readability_analysis_results = perform_readability_analysis(text)\n",
    "        basic_analysis_results = perform_basic_analysis(text)\n",
    "\n",
    "        # Update the DataFrame with sentiment analysis results\n",
    "        df.at[index, 'POSITIVE SCORE'] = sentiment_analysis_results['POSITIVE SCORE']\n",
    "        df.at[index, 'NEGATIVE SCORE'] = sentiment_analysis_results['NEGATIVE SCORE']\n",
    "        df.at[index, 'POLARITY SCORE'] = sentiment_analysis_results['POLARITY SCORE']\n",
    "        df.at[index, 'SUBJECTIVITY SCORE'] = sentiment_analysis_results['SUBJECTIVITY SCORE']\n",
    "        df.at[index, 'AVG SENTENCE LENGTH'] = readability_analysis_results['AVG SENTENCE LENGTH']\n",
    "        df.at[index, 'PERCENTAGE OF COMPLEX WORDS'] = readability_analysis_results['PERCENTAGE OF COMPLEX WORDS']\n",
    "        df.at[index, 'FOG INDEX'] = readability_analysis_results['FOG INDEX']\n",
    "        df.at[index, 'AVG NUMBER OF WORDS PER SENTENCE'] = basic_analysis_results['AVERAGE WORD PER SENTENCE']\n",
    "        df.at[index, 'COMPLEX WORD COUNT'] = basic_analysis_results['COMPLEX WORD COUNT']\n",
    "        df.at[index, 'WORD COUNT'] = basic_analysis_results['WORD COUNT']\n",
    "        df.at[index, 'SYLLABLE PER WORD'] = basic_analysis_results['AVERAGE SYLLABLE COUNT']\n",
    "        df.at[index, 'PERSONAL PRONOUNS'] = basic_analysis_results['PRONOUN COUNT']\n",
    "        df.at[index, 'AVG WORD LENGTH'] = basic_analysis_results['AVERAGE WORD LENGTH']\n",
    "\n",
    "        # Add more columns as needed\n",
    "# Write the updated DataFrame back to the Excel sheet\n",
    "df.to_excel(output_excel_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a0a668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7fbd84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4e6338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df44a61c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ff363b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf4f108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707b6120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
